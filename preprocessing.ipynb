{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc82e8ba3e5e8511",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset/data?select=phishing_email.csv\n",
    "\n",
    " \n",
    "\n",
    "### Project Objective:\n",
    "\n",
    "The objective of the project is to classify emails using supervised learning methods. Emails will be categorized into several classes such as:\n",
    "\n",
    "- Spam\n",
    "- Normal emails\n",
    "- Phishing\n",
    "- Fraud\n",
    "\n",
    "The dataset is composed of several separate sources:\n",
    "\n",
    "    Enron and Ling Datasets: primarily focused on the core content of emails.\n",
    "    CEAS, Nazario, Nigerian Fraud, and SpamAssassin Datasets: provide context about the message, such as the sender, recipient, date, etc.\n",
    "\n",
    "The data will require preprocessing to create a unified database that includes all necessary information. The entire project consists of approximately 85,000 emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4776fe39dcc018e",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The first part of preprocessing involves **importing** the necessary libraries and **NLTK packages** (a library for text processing) as well as **creating** a set of **stop words**—words that carry low informational value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cff9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_nltk() -> None:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "prepare_nltk()\n",
    "stop_words_set = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8d8f0",
   "metadata": {},
   "source": [
    "### Division into Different Categories\n",
    "\n",
    "The **final dataset** will consist of data from **6 different datasets**. Each of these datasets includes a specific category (label) that classifies a message as either **legit (label 0) or not (label 1)**. However, the datasets provide different types of information about the messages, and the **labels have different meanings depending on the dataset**. For this reason, each dataset has been assigned to a specific category:\n",
    "\n",
    "| Dataset name      | Category  | Label of faked messages |\n",
    "|-------------------|-----------|-------------------------|\n",
    "| **Enron**         | Spam      | **1**                  |\n",
    "| **Ling**          | Spam      | **1**                  |\n",
    "| **SpamAssasin**   | Spam      | **1**                  |\n",
    "| **CEAS_08**       | Phishing  | **2**                  |\n",
    "| **Nazario**       | Phishing  | **2**                  |\n",
    "| **Nigerian_Fraud**| Fraud     | **3**                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d334fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_spam_dataset = [\n",
    "    \"dataset/non-processed/SpamAssasin.csv\",\n",
    "    \"dataset/non-processed/Enron.csv\",\n",
    "    \"dataset/non-processed/Ling.csv\"\n",
    "]\n",
    "list_of_fraud_dataset = [\n",
    "    \"dataset/non-processed/Nigerian_Fraud.csv\"\n",
    "]\n",
    "list_of_phishing_dataset = [\n",
    "    \"dataset/non-processed/CEAS_08.csv\",\n",
    "    \"dataset/non-processed/Nazario.csv\"\n",
    "]\n",
    "\n",
    "\n",
    "final_data_frame = pd.DataFrame()\n",
    "\n",
    "for dataset_path in list_of_spam_dataset:\n",
    "\n",
    "    df = manage_dataset_preprocess(dataset_path)\n",
    "    df['label'] = df['label'].map({0: 0, 1: 1})\n",
    "    final_data_frame = pd.concat([final_data_frame, df], ignore_index=True)\n",
    "\n",
    "for dataset_path in list_of_phishing_dataset:\n",
    "    \n",
    "    df = manage_dataset_preprocess(dataset_path)\n",
    "    df['label'] = df['label'].map({0: 0, 1: 2})\n",
    "    final_data_frame = pd.concat([final_data_frame, df], ignore_index=True)\n",
    "\n",
    "\n",
    "for dataset_path in list_of_fraud_dataset:\n",
    "    \n",
    "    df = manage_dataset_preprocess(dataset_path)\n",
    "    df['label'] = df['label'].map({0: 0, 1: 3})\n",
    "    final_data_frame = pd.concat([final_data_frame, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24728886",
   "metadata": {},
   "source": [
    "### General Data Preprocessing\n",
    "\n",
    "Each dataset contains different columns. All of them include the **subject** and **body** columns, but some do not include the **receiver** and **sender** columns. If a dataset is missing one of these columns, the corresponding values are set to `'None'`. The final dataset will include the following five columns:\n",
    "\n",
    "- **body**\n",
    "- **subject**\n",
    "- **receiver**\n",
    "- **sender**\n",
    "- **label**\n",
    "\n",
    "Each of these columns will be processed by a dedicated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_dataset_preprocess(dataset_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    if 'sender' not in df.columns:\n",
    "        df['sender'] = None\n",
    "    if 'receiver' not in df.columns:\n",
    "        df['receiver'] = None\n",
    "    \n",
    "    df = df[['subject', 'body', 'sender', 'receiver',  'label']]\n",
    "    df['body'] = df['body'].apply(preprocess_dataset_text)\n",
    "    df['subject'] = df['subject'].apply(preprocess_dataset_subject)\n",
    "    df['receiver'] = df['receiver'].apply(preprocess_dataset_sender_receiver)\n",
    "    df['sender'] = df['sender'].apply(preprocess_dataset_sender_receiver)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529e560",
   "metadata": {},
   "source": [
    "### Column Processing Methodology\n",
    "\n",
    "Each string in the columns (excluding 'label') is **processed using regex** to reduce the amount of data and eliminate unnecessary noise. The table below summarizes the processing steps for each column:\n",
    "\n",
    "| Column            | Delete E-mail | Delete Links | Only Chars | Delete Garbage | Lowercase | Lemmatizer |\n",
    "|-------------------|---------------|--------------|------------|----------------|-----------|------------|\n",
    "| **Body**          | ✅             | ✅            | ✅          | ✅              | ✅         | ✅          |\n",
    "| **Subject**       | ❌             | ❌            | ✅          | ❌              | ✅         | ✅          |\n",
    "| **Sender**, **Receiver** | ❌       | ❌            | ❌          | ❌              | ✅         | ❌          |\n",
    "\n",
    "#### Definitions:\n",
    "- **Garbage word**: A word with four consecutive repetitions of the same letter or longer than 15 characters. These are removed only from the **'body'** column, where reducing data size is most critical.\n",
    "- **Stop words**: Frequently used words with low informational value are removed from the **'body'** and **'subject'** columns (discussed in a later section).\n",
    "- **Sender** and **Receiver**: Allowed characters are `'.'`, `'-'`, and `'_'`. In these columns, the **username** of the email address is stripped, leaving only the **domain name** and **top-level domain**.\n",
    "\n",
    "If the input data **is not a string** or the processed string **is empty**, the value is returned as **'None'**.\n",
    "\n",
    "#### Code for Processing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset_text(text: str) -> str | None:\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    \n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    ### Remove garbage\n",
    "    text = re.sub(r'\\b([a-zA-Z])\\1{4,}\\b', '', text)  # Same letter more than 5 times\n",
    "    text = re.sub(r'\\b\\w{15,}\\b', '', text)  # Words longer than 15 chars\n",
    "\n",
    "    text = text.lower()\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words_set)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    if text == '':\n",
    "        return None\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_dataset_subject(subject: str) -> str | None:\n",
    "    \n",
    "    if not isinstance(subject, str) or subject == '':\n",
    "        return None\n",
    "    \n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    subject = subject.lower()\n",
    "    subject = re.sub(r'[^a-zA-Z\\s]', '', subject)\n",
    "\n",
    "    # Stop_words\n",
    "    subject = \" \".join(word for word in subject.split() if word not in stop_words_set)\n",
    "    subject = \" \".join([lemmatizer.lemmatize(word) for word in subject.split()])  # 4. Lematyzacja\n",
    "\n",
    "    if subject == '':\n",
    "        return None\n",
    "\n",
    "    return subject\n",
    "\n",
    "\n",
    "def preprocess_dataset_sender_receiver(data: str) -> str | None:\n",
    "    \n",
    "    if not isinstance(data, str):\n",
    "        return None\n",
    "\n",
    "    data = data.strip().lower()\n",
    "    data = data.split('@')[-1]        # TODO: Only domain\n",
    "\n",
    "    match = re.match(r'^[a-z0-9._-]*', data)\n",
    "\n",
    "    if match:\n",
    "        data = match.group(0)\n",
    "\n",
    "    if data == '':\n",
    "            return None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e284c",
   "metadata": {},
   "source": [
    "### Handling Empty or 'None' Values\n",
    "\n",
    "Some datasets initially lacked certain required data, as shown below:\n",
    "\n",
    "| Dataset            | Body | Subject | Sender | Receiver |\n",
    "|--------------------|------|---------|--------|----------|\n",
    "| **Enron**          | ✅    | ✅       | ❌      | ❌        |\n",
    "| **Ling**           | ✅    | ✅       | ❌      | ❌        |\n",
    "| **SpamAssasin**    | ✅    | ✅       | ✅      | ✅        |\n",
    "| **CEAS_08**        | ✅    | ✅       | ✅      | ✅        |\n",
    "| **Nazario**        | ✅    | ✅       | ✅      | ✅        |\n",
    "| **Nigerian_Fraud** | ✅    | ✅       | ✅      | ✅        |\n",
    "\n",
    "Additionally, some data was lost during filtering and column processing (all empty values were replaced with `'None'`). To address this, missing values (`'None'`) will be replaced by the **most frequently occurring value** (mode) in each column.\n",
    "\n",
    "#### Code for Imputing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8640e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df: pd.DataFrame, column: str) -> None:\n",
    "    mode_value = df[column].mode()[0]\n",
    "    df.fillna({column: mode_value}, inplace=True)\n",
    "\n",
    "\n",
    "impute_missing_values(final_data_frame, \"subject\")\n",
    "impute_missing_values(final_data_frame, \"body\")\n",
    "impute_missing_values(final_data_frame, \"sender\")\n",
    "impute_missing_values(final_data_frame, \"receiver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effbcb65",
   "metadata": {},
   "source": [
    "### Final Datasets\n",
    "\n",
    "The processed data will be saved as `.csv` files. Since the project focuses on testing various models with different characteristics, four distinct datasets were created:\n",
    "\n",
    "- **final.csv**\n",
    "- **final-domain-only.csv**\n",
    "- **final-with-stop-words.csv**\n",
    "- **final-with-stop-words-domain-only.csv**\n",
    "\n",
    "#### Transformations Applied:\n",
    "\n",
    "| Final Dataset Name               | Transform 'sender' and 'receiver' to domain format | Include Stop Words |\n",
    "|----------------------------------|----------------------------------------------------|--------------------|\n",
    "| **final.csv**                    | ❌                                                  | ❌                 |\n",
    "| **final-domain-only.csv**        | ✅                                                  | ❌                 |\n",
    "| **final-with-stop-words.csv**    | ❌                                                  | ✅                 |\n",
    "| **final-with-stop-words-domain-only.csv** | ✅                                          | ✅                 |\n",
    "\n",
    "Each of these datasets will be compared across all models to evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc24ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_frame.to_csv(\"dataset/processed/final-domain-only.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f958f3",
   "metadata": {},
   "source": [
    "## Selected Models\n",
    "\n",
    "Three different models from distinct categories were chosen for training to evaluate which model performs best for categorization within its category. The selected models are:\n",
    "\n",
    "- **XGBoost**: A gradient boosting model utilizing an ensemble of decision trees. It is highly efficient and excels in structured data tasks.\n",
    "- **LSTM**: A recurrent neural network model designed to handle sequential data. It is particularly well-suited for tasks involving temporal dependencies, such as text processing.\n",
    "- **BERT**: A transformer-based model pre-trained on a large corpus of text. BERT excels in understanding context and meaning in text, making it ideal for natural language processing tasks.\n",
    "\n",
    "### Purpose of Model Selection\n",
    "The diversity of these models allows for a comprehensive comparison, highlighting strengths and weaknesses based on the type of data and categorization task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
