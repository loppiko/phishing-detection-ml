{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92260daf",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "MAIN_DIR = \"/kaggle/working/\"\n",
    "\n",
    "\n",
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, *args) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, *args) -> float:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_plots(self, *args) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_confusion_matrix(self, *args) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997d5cb",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7d1b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "class F1ScoreCallbackBert(Callback):\n",
    "    def __init__(self, val_dataset):\n",
    "        val_data = list(val_dataset.as_numpy_iterator())\n",
    "        \n",
    "        self.x_val = {key: np.concatenate([batch[0][key] for batch in val_data]) for key in val_data[0][0].keys()}\n",
    "        self.y_val = np.concatenate([batch[1] for batch in val_data])\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred_logits = self.model.predict(self.x_val).logits\n",
    "        \n",
    "        y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "        \n",
    "        f1 = f1_score(self.y_val, y_pred, average='weighted')\n",
    "        self.f1_scores.append(f1)\n",
    "\n",
    "    def _implements_train_batch_hooks(self):\n",
    "        return False\n",
    "\n",
    "    def _implements_test_batch_hooks(self):\n",
    "        return False\n",
    "\n",
    "    def _implements_predict_batch_hooks(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1301a67",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import win32_edition\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.python.data.ops.dataset_ops import DatasetV2\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6bbcd",
   "metadata": {},
   "source": [
    "### Model Parameters Description\n",
    "\n",
    "- **model**: The pre-trained model that will be fine-tuned.\n",
    "- **tokenizer**: Loads a tokenizer that is compatible with the model type (WordPiece Tokenization). It tokenizes sub-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"bert-base-uncased\"\n",
    "\n",
    "class BertModel():\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerFast\n",
    "    history: History\n",
    "    cm: list[list[float]] | None = None\n",
    "\n",
    "    def __init__(self, model_type: str, model_name: str) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model = TFAutoModelForSequenceClassification.from_pretrained(model_type, num_labels=4)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1f3b6",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f52fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(self, data: list[list[str]], max_length: int):\n",
    "        return self.tokenizer(\n",
    "            data,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807a17f",
   "metadata": {},
   "source": [
    "### Training and precictions\n",
    "\n",
    "Podczas trenowania modelu, zostały przetestowane dwa różne podejścia, lecz niestety nie przyniosły one spodziewanych rezultatów.\n",
    "\n",
    "- Tak jak w poprzednich modelach zostały najpierw użyte wagi, aby wyrównać nierówność klas, lecz model ten nie reagował na to podejście\n",
    "- Po wielu nieudanych próbach użycia wag, podejście zostało zmienione na oversampling danych, lecz niestety także i to okazało się bezskuteczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, train_dataset: DatasetV2, val_dataset: DatasetV2, class_weights_dict: dict | None = None) -> None:\n",
    "        # labels_array = []\n",
    "        # for _, label in train_dataset:\n",
    "        #     labels_array.extend(label.numpy())  # Get labels from tensor\n",
    "    \n",
    "        # Converting to numpy\n",
    "        # labels_array = np.array(labels_array)\n",
    "    \n",
    "        # Calculating weights\n",
    "        # class_weights = compute_class_weight(\n",
    "        #     class_weight='balanced',\n",
    "        #     classes=np.unique(labels_array),\n",
    "        #     y=labels_array\n",
    "        # )\n",
    "    \n",
    "        # Convert to dict and use them in model\n",
    "        # class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "        # class_weights_dict = {0: 0.4, 1: 1.1, 2: 0.9, 3: 11}\n",
    "\n",
    "        print(\"Class weights:\", class_weights_dict)\n",
    "    \n",
    "        self.model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "        f1_callback = F1ScoreCallbackBert(val_dataset)\n",
    "    \n",
    "        self.history = self.model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=30,\n",
    "            callbacks=[f1_callback]\n",
    "        )\n",
    "    \n",
    "        self.f1_scores = f1_callback.f1_scores\n",
    "\n",
    "\n",
    "\n",
    "def predict(self, to_predict: list[list[str]]) -> np.ndarray:\n",
    "        tokenized_data = self.tokenize_data(to_predict, max_length=250)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(dict(tokenized_data)).batch(8)\n",
    "        logits = self.model.predict(test_dataset).logits\n",
    "\n",
    "        # Konwersja logitów na etykiety (największa wartość logitu dla każdej próbki)\n",
    "        return tf.argmax(logits, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909eb4b",
   "metadata": {},
   "source": [
    "### Making plots and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72724caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, test_dataset: DatasetV2) -> None:\n",
    "        loss, accuracy = self.model.evaluate(test_dataset)\n",
    "        print(f\"loss: {loss}, accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "def create_confusion_matrix(self, x_test: list[list[str]], y_test: np.ndarray) -> None:\n",
    "        y_pred = self.predict(x_test)\n",
    "        self.cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "def make_plots(self) -> None:\n",
    "        history = self.history.history\n",
    "\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Accuracy Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.f1_scores, label='Validation F1-score')\n",
    "        plt.title('F1-score Over Epochs')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1-score')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{MAIN_DIR}{self.model_name}-accuracy.png\")\n",
    "\n",
    "        if self.cm is not None:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            labels = list(self.model.config.id2label.values()) if hasattr(self.model.config, \"id2label\") else None\n",
    "            sns.heatmap(self.cm, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=['legal', 'spam', 'phishing', 'fraud'],\n",
    "                        yticklabels=['legal', 'spam', 'phishing', 'fraud'])\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.savefig(f\"{MAIN_DIR}{self.model_name}-confusion-matrix.png\")\n",
    "        else:\n",
    "            print(\"Cannot save confusion-matrix, because it was not previously created\")\n",
    "\n",
    "        with open(f\"{MAIN_DIR}{self.model_name}-final-metrics.txt\", \"w\") as file:\n",
    "            file.write(f\"Accuracy: {history['accuracy'][-1]}\\n\")\n",
    "            file.write(f\"F1-Score: {self.f1_scores[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475650a",
   "metadata": {},
   "source": [
    "### Running model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_model(self, train_data: pd.DataFrame, train_labels: pd.DataFrame, max_length: int) -> None:\n",
    "        X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
    "            train_data.values.tolist(), \n",
    "            train_labels.values.tolist(), \n",
    "            test_size=0.3, \n",
    "            random_state=28\n",
    "        )\n",
    "        X_val, X_test, Y_val, Y_test = train_test_split(\n",
    "            X_temp, Y_temp, test_size=0.5, random_state=28\n",
    "        )\n",
    "    \n",
    "        # Oversampling\n",
    "        oversampler = RandomOverSampler(random_state=40)\n",
    "        X_train_resampled, Y_train_resampled = oversampler.fit_resample(np.array(X_train).reshape(-1, 1), Y_train)\n",
    "        X_val_resampled, Y_val_resampled = oversampler.fit_resample(np.array(X_val).reshape(-1, 1), Y_val)\n",
    "\n",
    "        \n",
    "        X_train_resampled = X_train_resampled.flatten().tolist()\n",
    "        X_val_resampled = X_val_resampled.flatten().tolist()\n",
    "        # Data tokenization\n",
    "        X_test_df: list[list[str]] = X_test\n",
    "        X_train, X_val, X_test = (\n",
    "            self.tokenize_data(X_train_resampled, max_length),\n",
    "            self.tokenize_data(X_val_resampled, max_length),\n",
    "            self.tokenize_data(X_test, max_length)\n",
    "        )\n",
    "    \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), Y_train_resampled)).batch(8)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((dict(X_val), Y_val_resampled)).batch(8)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((dict(X_test), Y_test)).batch(8)\n",
    "    \n",
    "        # Calculating weights\n",
    "        # class_weights = compute_class_weight(\n",
    "        #     class_weight='balanced', \n",
    "        #     classes=np.unique(Y_train_resampled), \n",
    "        #     y=Y_train_resampled\n",
    "        # )\n",
    "        # class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    \n",
    "        self.fit(train_dataset, val_dataset)\n",
    "    \n",
    "        self.evaluate(test_dataset)\n",
    "        self.create_confusion_matrix(X_test_df, np.array(Y_test))\n",
    "        self.make_plots()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MODELS_PARAMS = [\n",
    "    {\"model_name\": \"only-body\", \"dataset\": \"/kaggle/input/final-2-csv/final.csv\", \"add_subject\": False, \"add_domain\": False},\n",
    "    {\"model_name\": \"stop-words-body\", \"dataset\": \"/kaggle/input/final-2-csv/final-with-stop-words.csv\", \"add_subject\": False, \"add_domain\": False},\n",
    "    {\"model_name\": \"body-subject-stop\", \"dataset\": \"/kaggle/input/final-2-csv/final-with-stop-words.csv\", \"add_subject\": True, \"add_domain\": False},\n",
    "    {\"model_name\": \"body-domain-stop\", \"dataset\": \"/kaggle/input/final-2-csv/final-with-stop-words-domain-only.csv\", \"add_subject\": False, \"add_domain\": True},\n",
    "    {\"model_name\": \"full-data-stop\", \"dataset\": \"/kaggle/input/final-2-csv/final-with-stop-words-domain-only.csv\", \"add_subject\": True, \"add_domain\": True}\n",
    "]\n",
    "\n",
    "\n",
    "for model_param in MODELS_PARAMS:\n",
    "    data = pd.read_csv(f\"/kaggle/input/final-2-csv/final.csv\").sample(n=16000, random_state=29)\n",
    "    body = data['body']\n",
    "    labels = data['label']\n",
    "    max_length = 250\n",
    "    \n",
    "    bert_model = BertModel(MODEL_TYPE, model_param[\"model_name\"])\n",
    "    bert_model.train_bert_model(body, labels, 250)\n",
    "    bert_model.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
